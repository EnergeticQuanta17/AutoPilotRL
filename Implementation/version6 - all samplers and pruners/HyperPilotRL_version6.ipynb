{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-b7acvpxWm-",
        "outputId": "fc2eea55-6b51-4d71-e44c-9cca8a2eab8d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stable_baselines3"
      ],
      "metadata": {
        "id": "NvODkfx50Fsp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7uHW8CIimsHi",
        "outputId": "c723c1d2-2800-4002-f063-5c363681b6aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.9/dist-packages (3.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (1.4.47)\n",
            "Requirement already satisfied: cmaes>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from optuna) (0.9.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (1.10.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.9/dist-packages (from optuna) (6.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.9/dist-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.9/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.9/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#   **LOADER**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pm3lFX0u0iHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from stable_baselines3 import PPO\n",
        "import gym\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MegaLoader:\n",
        "    def __init__(self, env_name, algo, directory):\n",
        "        self.env_name = env_name\n",
        "        self.env = gym.make(self.env_name)\n",
        "        self.env.reset()\n",
        "\n",
        "        self.algorithm = algo\n",
        "        self.directory = directory\n",
        "\n",
        "    def model_selector(self, all_models_dir):\n",
        "        # print( os.listdir(all_models_dir)[-1])\n",
        "        return os.listdir(all_models_dir)[-1]\n",
        "\n",
        "        # return whole file name properly, with date and all\n",
        "        pass\n",
        "\n",
        "    def load(self, execution_number, no_of_episodes):\n",
        "        # Input :- \n",
        "        #    1. execution_number - \n",
        "        #    2. no_of_episodes - \n",
        "        #    3. \n",
        "\n",
        "        # Returns :-\n",
        "        #    Dictionary with the following items\n",
        "            #    1. total_return       [int]\n",
        "            #    2. return_per_episode [list]\n",
        "            #    3.\n",
        "            #    3.\n",
        "            #    3.\n",
        "            #    3.\n",
        "\n",
        "        total_return = 0\n",
        "        return_per_episode = []\n",
        "        info_per_episode = []\n",
        "\n",
        "        all_models_dir = f\"model/{self.directory}/{execution_number}\"\n",
        "\n",
        "        model_no = self.model_selector(all_models_dir)\n",
        "        model_dir = f\"{all_models_dir}/{model_no}\"\n",
        "\n",
        "        model = PPO.load(model_dir, env=self.env)\n",
        "\n",
        "        for ep in range(no_of_episodes):\n",
        "            obs = self.env.reset()\n",
        "            done = False\n",
        "            info = \"\"\n",
        "            episode_return = 0\n",
        "            \n",
        "            while not done:\n",
        "                action, _states = model.predict(obs)\n",
        "                obs, rewards, done, info = self.env.step(action)\n",
        "                episode_return += rewards\n",
        "\n",
        "                # self.env.render()\n",
        "\n",
        "            return_per_episode.append(episode_return)\n",
        "            total_return += episode_return\n",
        "            info_per_episode.append(info)\n",
        "\n",
        "        model_test_info = {\n",
        "            \"total_return\": total_return,\n",
        "            \"return_per_episode\": return_per_episode,\n",
        "            \"info_per_episode\": info_per_episode,\n",
        "        }\n",
        "\n",
        "        return model_test_info"
      ],
      "metadata": {
        "id": "DGUyJR-5mk9U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AqddTPk7mxdz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PT9-qV4H06Ex"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#   **TRAINER**"
      ],
      "metadata": {
        "id": "jm0eEt0G06yU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime\n",
        "import gym\n",
        "import json\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "import optuna\n",
        "\n",
        "class MegaTrainer:\n",
        "    def __init__(self, env_name, algo):\n",
        "        with open(\"previous_request.json\", \"r\") as f:\n",
        "            self.data = json.loads(f.read())\n",
        "\n",
        "        self.env_name = env_name\n",
        "        self.algorithm = algo\n",
        "        self.counter = self.data[\"counter\"] + 1\n",
        "\n",
        "        self.env = gym.make(self.env_name)\n",
        "        self.env.reset()\n",
        "\n",
        "        self.data[\"counter\"] += 1\n",
        "        with open(\"previous_request.json\", \"w\") as f:\n",
        "            json.dump(self.data, f)\n",
        "        \n",
        "    def make_directories(self, models_dir, logdir):\n",
        "        if(not os.path.exists(models_dir)):\n",
        "            os.makedirs(models_dir)\n",
        "\n",
        "        if(not os.path.exists(logdir)):\n",
        "            os.makedirs(logdir)\n",
        "\n",
        "    def learn(self, timestep, iterations, hyps, directory, trial):\n",
        "        model_dir = f\"model/{directory}/{self.counter}\"\n",
        "        logdir = f\"logs/{directory}\"\n",
        "        self.make_directories(model_dir, logdir)\n",
        "\n",
        "        model = PPO(**hyps)\n",
        "\n",
        "        for i in range(1, iterations+1):\n",
        "            if(trial.should_prune()):\n",
        "                raise optuna.TrialPruned()\n",
        "            dt = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S-%f\")\n",
        "            model.learn(total_timesteps=timestep, reset_num_timesteps=False, tb_log_name=str(self.counter))\n",
        "            model.save(model_dir+f\"/{dt}_{timestep*i}\")\n",
        "\n",
        "        return self.counter"
      ],
      "metadata": {
        "id": "UHvH88yJ0-1U"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzGjRixK0_dU",
        "outputId": "2336c01c-4b29-41a0-91b6-261ac3e1577d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#   **HYPERPARMETER CONFIGURATIONS**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FR0P1ntw1AOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import optuna\n",
        "import numpy as np\n",
        "\n",
        "## change default to \"selector\" for that parameter\n",
        "## there are specifications for what type should be there, see that and do final design. FOr exxample --> learning_rate (Union[float, Callable[[float], float]])\n",
        "## Fix TENSORBOARD_LOG\n",
        "## Generate a number/code that is unique to each hyperparmeter configuration\n",
        "\n",
        "\n",
        "\n",
        "# import stable_baselines3.common.schedules as schedules\n",
        "\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "##             LEARNING RATE SCHEDULER (learning_rate)             ##\n",
        "#####################################################################\n",
        "# Currently random, make it categorical and pass parameters using dictionary properly\n",
        "\n",
        "class Learning_Rate_Scheduler:\n",
        "    # remove this default function later\n",
        "    # Make this categorical\n",
        "    # send params in __init__ for all class's functions --> def __init__(self, i, params):\n",
        "    # always call through \n",
        "    \n",
        "\n",
        "    def __init__(self):\n",
        "        self.low = 1e-5\n",
        "        self.high = 1e-2\n",
        "        self.log = True\n",
        "        #do this for all\n",
        "    \n",
        "    def default(self):\n",
        "        return random.random() / 1000\n",
        "\n",
        "    def opt(self, trial):\n",
        "        return trial.suggest_float(\"learning_rate\", self.low, self.high, log=self.log)\n",
        "\n",
        "    def step_decay(optimizer, initial_lr=0.001, drop_rate=0.1, epochs_drop=10):\n",
        "        return\n",
        "        scheduler = lr_scheduler.StepLR(optimizer, step_size=epochs_drop, gamma=drop_rate)\n",
        "        return scheduler\n",
        "\n",
        "    def exp_decay(optimizer, initial_lr=0.001, decay_rate=0.96):\n",
        "        return\n",
        "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=decay_rate)\n",
        "        return scheduler\n",
        "\n",
        "    def cosine_anneal(optimizer, initial_lr=0.001, epochs=100):\n",
        "        return\n",
        "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "        return scheduler\n",
        "\n",
        "    def reduce_on_plateau(optimizer, mode='min', factor=0.1, patience=10, verbose=False):\n",
        "        return\n",
        "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=mode, factor=factor, patience=patience, verbose=verbose)\n",
        "        return scheduler\n",
        "\n",
        "    def cyclical_lr(optimizer, step_size=2000, base_lr=1e-3, max_lr=6e-3, mode='triangular', gamma=1.):\n",
        "        return\n",
        "        scheduler = lr_scheduler.CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, step_size_up=step_size,\n",
        "                                        mode=mode, gamma=gamma)\n",
        "        return scheduler\n",
        "\n",
        "    def warmup_lr(optimizer, factor=10, warmup_epochs=5):\n",
        "        return\n",
        "        def lr_lambda(current_epoch):\n",
        "            if current_epoch < warmup_epochs:\n",
        "                return factor\n",
        "            else:\n",
        "                return 1.0\n",
        "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "        return scheduler\n",
        "\n",
        "    def one_cycle_lr(optimizer, num_steps, lr_range=(1e-4, 1e-2), momentum_range=(0.85, 0.95)):\n",
        "        return\n",
        "        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=lr_range[1], total_steps=num_steps,\n",
        "                                            pct_start=0.3, anneal_strategy='cos', cycle_momentum=True,\n",
        "                                            base_momentum=momentum_range[0], max_momentum=momentum_range[1])\n",
        "        return scheduler\n",
        "\n",
        "    def constant_schedule(self, lr=3e-4):\n",
        "        return schedules.constant_schedule(3e-4)\n",
        "\n",
        "    def linear_schedule(self, initial_value=1e6, final_value=3e-4, n_timsteps=1e-4):\n",
        "        return schedules.linear_schedule(initial_value, final_value, n_timsteps)\n",
        "\n",
        "    def piecewise_schedule(self, schedule_pieces):\n",
        "        return schedules.piecewise_schedule(schedule_pieces)\n",
        "\n",
        "    def cosine_schedule(self, initial_lr=3e-4, final_lr=1e-4, total_timesteps=1e6): # check if this cosine annealing\n",
        "        return schedules.cosine_schedule(initial_lr, final_lr, total_timesteps)\n",
        "\n",
        "    def linear_warmup_schedule(self):\n",
        "        warmup_timesteps = 1e5\n",
        "        learning_rate_schedule = schedules.linear_schedule(9e5, 0, 3e-4)\n",
        "        learning_rate = schedules.linear_warmup(learning_rate_schedule, warmup_timesteps, 0)\n",
        "        return learning_rate\n",
        "\n",
        "    def combined_scheduler(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "##                        POLICY(policy)                           ##\n",
        "#####################################################################\n",
        "# Categorical\n",
        "class PolicySelector:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def opt(self, trial):\n",
        "        return \"MlpPolicy\"\n",
        "        \n",
        "    def default(self):\n",
        "        return self.MlpPolicy()\n",
        "\n",
        "    def MlpPolicy(self):\n",
        "        return \"MlpPolicy\"\n",
        "    \n",
        "    def CnnPolicy(self):\n",
        "        return \"CnnPolicy\"\n",
        "\n",
        "    def MultiInputPolicy(self):\n",
        "        return \"MultiInputPolicy\"\n",
        "    \n",
        "\n",
        "#####################################################################\n",
        "##                  UPDATE PER STEPS (n_steps)                     ##\n",
        "#####################################################################\n",
        "#Refine the search space --> dont just do 2^i in between also there might be peak\n",
        "\n",
        "class StepsPerUpdate:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.max_size = 2048\n",
        "\n",
        "        pass\n",
        "\n",
        "    def opt(self, trial):\n",
        "        return 2 ** trial.suggest_int(\"n_steps\", 10, 12)\n",
        "\n",
        "    def default(self):\n",
        "        return random.randint(1, self.max_size)\n",
        "\n",
        "#####################################################################\n",
        "##                      BATCH SIZE (batch_size)                    ##\n",
        "#####################################################################\n",
        "\n",
        "class BatchSize:\n",
        "    def __init__(self):\n",
        "        self.max_size = 64\n",
        "        pass\n",
        "\n",
        "    def opt(self, trial):\n",
        "        return 2 ** trial.suggest_int(\"batch_size\", 5, 8)\n",
        "    \n",
        "    def default(self):\n",
        "        self.max_size=26\n",
        "        return random.randint(1, self.max_size)\n",
        "\n",
        "#####################################################################\n",
        "##    No. OF EPOCHS WHEN OPTIMIZING SURROGATE LOSS (n_epochs)      ##\n",
        "#####################################################################\n",
        "# Higher the better\n",
        "\n",
        "class NoOfEpochs:\n",
        "    def __init__(self):\n",
        "        self.max_size = 10\n",
        "        pass\n",
        "\n",
        "    def opt(self, trial):\n",
        "        return trial.suggest_int(\"n_epochs\", 5, 10)\n",
        "    \n",
        "    def default(self):\n",
        "        return random.randint(1, self.max_size)\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "##                      DISCOUNT FACTOR (gamma)                    ##\n",
        "#####################################################################\n",
        "\n",
        "class DiscountFactor:\n",
        "    def __init__(self):\n",
        "        self.max_size = 10\n",
        "    \n",
        "    def opt(self, trial):\n",
        "        return trial.suggest_float(\"gamma\", 0.9, 0.999)\n",
        "    \n",
        "    def default(self):\n",
        "        return random.random()\n",
        "\n",
        "#####################################################################\n",
        "##    BIAS-VARIANCE TRADEOFF for GENERALIZED ADVANTAGE ESTIMATOR   ##\n",
        "#####################################################################\n",
        "\n",
        "class BiasVarianceTradeoff:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def opt(self, trial):\n",
        "        return trial.suggest_float(\"bvtradeoff\", 0.9, 0.99)\n",
        "    \n",
        "    def default(self):\n",
        "        return random.random()\n",
        "\n",
        "#####################################################################\n",
        "##                  CLIPPING FUNCTION (clip_range)                 ##\n",
        "#####################################################################\n",
        "# Can be a function of progress\n",
        "\n",
        "class ClipRange:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def opt(self, trial):\n",
        "        return trial.suggest_float(\"clip_range\", 0.1, 0.3)\n",
        "    \n",
        "    def default(self):\n",
        "        return random.random()\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "##       CLIPPING RANGE for Value Function (clip_range_vf)         ##\n",
        "#####################################################################\n",
        "\n",
        "class ClipRangeVF:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def opt(self, trial):\n",
        "        return trial.suggest_float(\"clip_range_vf\", 0.1, 0.3)\n",
        "    \n",
        "    def default(self):\n",
        "        return random.random()\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "##             NORMALIZE ADVANTAGE(normalize_advantage)            ##\n",
        "#####################################################################\n",
        "\n",
        "class NormalizeAdvantage:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def opt(self, trial):\n",
        "        return False\n",
        "\n",
        "    def default(self):\n",
        "        return True\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "##                  ENTROPY COEFFICIENT (ent_coef)                 ##\n",
        "#####################################################################\n",
        "\n",
        "class EntropyCoefficient:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def opt(self, trial):\n",
        "        return trial.suggest_float(\"ent_coef\", 0, 0.01)\n",
        "    \n",
        "    def default(self):\n",
        "        return 0\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "##            VALUE FUNCTION COEFFICIENT (vf_coef)                 ##\n",
        "#####################################################################\n",
        "\n",
        "class ValueFunctionCoefficient:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def opt(self, trial):\n",
        "        return trial.suggest_float(\"vf_coef\", 0.25, 0.75)\n",
        "    \n",
        "    def default(self):\n",
        "        return 0.5\n",
        "\n",
        "#####################################################################\n",
        "##       MAXIMUM VALUE for Gradient Clipping (max_grad_norm)       ##\n",
        "#####################################################################\n",
        "\n",
        "class MaxGradNorm:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def opt(self, trial):\n",
        "        return trial.suggest_float(\"max_grad_norm\", 0.3, 1)\n",
        "    \n",
        "    def default(self):\n",
        "        return 0.5\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "##        USE ? Generalized State Dependent Exploration            ##\n",
        "#####################################################################\n",
        "\n",
        "class BoolGeneralizedStateDependentExploration:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def opt(self, trial):\n",
        "        return False\n",
        "    \n",
        "    def default(self):\n",
        "        return False\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "##             SAMPLE NEW NOISE MATRIX(sde_sample_freq)            ##\n",
        "#####################################################################\n",
        "\n",
        "class SDESampleFrequency:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def opt(self, trial):\n",
        "        return -1\n",
        "    \n",
        "    def default(self):\n",
        "        return -1\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "##                LIMIT KL-Divergence (target_kl)                  ##\n",
        "#####################################################################\n",
        "\n",
        "class TargetKL:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def opt(self, trial):\n",
        "        return trial.suggest_float(\"target_kl\", 0.01, 0.05)\n",
        "    \n",
        "    def default(self):\n",
        "        return None\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "##                      TENSORBOARD_LOG                            ##\n",
        "#####################################################################\n",
        "\n",
        "# class TensorBoardLog:\n",
        "#     def __init__(self):\n",
        "#         pass\n",
        "\n",
        "#     def opt(self, trial):\n",
        "#         return \n",
        "    \n",
        "#     def default(self):\n",
        "#         return None\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "##                   POLICY_KWARGS (policy_kwargs)                 ##\n",
        "#####################################################################\n",
        "\n",
        "class PolicyKWargs:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def opt(self, trial):\n",
        "        return None\n",
        "    \n",
        "    def default(self):\n",
        "        return None\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "##                           SEED (seed)                           ##\n",
        "#####################################################################\n",
        "\n",
        "class Seed:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def opt(self, trial):\n",
        "        return trial.suggest_int(\"seed\", 1, 10)\n",
        "    \n",
        "    def default(self):\n",
        "        return None\n",
        "\n",
        "\n",
        "all_policy = [\n",
        "    \"MlpPolicy\",\n",
        "    \"CnnPolicy\",\n",
        "    \"MultiInputPolicy\",\n",
        "]\n",
        "\n",
        "# ~!@ environment is fixed {maybe we can find stronger relations between hyperparameters across other environments}\n",
        "    # Very important idea\n",
        "\n",
        "\n",
        "all_classes_in_this_file = [\n",
        "    'BatchSize',\n",
        "    'BiasVarianceTradeoff',\n",
        "    'BoolGeneralizedStateDependentExploration',\n",
        "    'ClipRange',\n",
        "     'ClipRangeVF',\n",
        "      'DiscountFactor',\n",
        "       'EntropyCoefficient',\n",
        "        'Learning_Rate_Scheduler',\n",
        "         'MaxGradNorm',\n",
        "          'MegaHandler',\n",
        "           'NoOfEpochs',\n",
        "            'NormalizeAdvantage',\n",
        "             'PolicyKWargs',\n",
        "              'PolicySelector',\n",
        "               'SDESampleFrequency',\n",
        "                'Seed',\n",
        "                 'StepsPerUpdate',\n",
        "                  'TargetKL',\n",
        "                   'TensorBoardLog',\n",
        "                    'ValueFunctionCoefficient'\n",
        "]\n",
        "\n",
        "def request_next_HypConfig(env, tb_logs):\n",
        "    hyps = dict()\n",
        "\n",
        "    hyps[\"policy\"] = PolicySelector().default()\n",
        "    hyps[\"env\"] = env\n",
        "    hyps[\"learning_rate\"] = Learning_Rate_Scheduler().default()\n",
        "    hyps[\"n_steps\"] = StepsPerUpdate().default()\n",
        "    hyps[\"batch_size\"] = BatchSize().default()\n",
        "    hyps[\"n_epochs\"] = NoOfEpochs().default()\n",
        "    hyps[\"gamma\"] = DiscountFactor().default()\n",
        "    hyps[\"gae_lambda\"] = BiasVarianceTradeoff().default()\n",
        "    hyps[\"clip_range\"] = ClipRange().default()\n",
        "    hyps[\"clip_range_vf\"] = ClipRangeVF().default()\n",
        "    hyps[\"normalize_advantage\"] = NormalizeAdvantage().default()\n",
        "    hyps[\"ent_coef\"] = EntropyCoefficient().default()\n",
        "    hyps[\"vf_coef\"] = ValueFunctionCoefficient().default()\n",
        "    hyps[\"max_grad_norm\"] = MaxGradNorm().default()\n",
        "    hyps[\"use_sde\"] = BoolGeneralizedStateDependentExploration().default()\n",
        "    hyps[\"sde_sample_freq\"] = SDESampleFrequency().default()\n",
        "    hyps[\"target_kl\"] = TargetKL().default()\n",
        "    hyps[\"tensorboard_log\"] = tb_logs\n",
        "    hyps[\"policy_kwargs\"] = PolicyKWargs().default()\n",
        "    hyps[\"verbose\"] = 0\n",
        "    hyps[\"seed\"] = Seed().default()\n",
        "    hyps[\"device\"] = 'auto'\n",
        "    hyps[\"_init_setup_model\"] = True\n",
        "\n",
        "    return hyps\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "##            MEGA HANDLER (handler of all classes below)          ##\n",
        "#####################################################################\n",
        "class MegaHandler:\n",
        "    def __init__(slef):\n",
        "        pass\n",
        "\n",
        "    def request_next_HypConfig(self, trial, env, tb_logs):\n",
        "        hyps = dict()\n",
        "\n",
        "        hyps[\"policy\"] = PolicySelector().opt(trial)\n",
        "        hyps[\"env\"] = env\n",
        "        hyps[\"learning_rate\"] = Learning_Rate_Scheduler().opt(trial)\n",
        "        hyps[\"n_steps\"] = StepsPerUpdate().opt(trial)\n",
        "        hyps[\"batch_size\"] = BatchSize().opt(trial)\n",
        "        hyps[\"n_epochs\"] = NoOfEpochs().opt(trial)\n",
        "        hyps[\"gamma\"] = DiscountFactor().opt(trial)\n",
        "        hyps[\"gae_lambda\"] = BiasVarianceTradeoff().opt(trial)\n",
        "        hyps[\"clip_range\"] = ClipRange().opt(trial)\n",
        "        hyps[\"clip_range_vf\"] = ClipRangeVF().opt(trial)\n",
        "        hyps[\"normalize_advantage\"] = NormalizeAdvantage().opt(trial)\n",
        "        hyps[\"ent_coef\"] = EntropyCoefficient().opt(trial)\n",
        "        hyps[\"vf_coef\"] = ValueFunctionCoefficient().opt(trial)\n",
        "        hyps[\"max_grad_norm\"] = MaxGradNorm().opt(trial)\n",
        "        hyps[\"use_sde\"] = BoolGeneralizedStateDependentExploration().opt(trial)\n",
        "        hyps[\"sde_sample_freq\"] = SDESampleFrequency().opt(trial)\n",
        "        hyps[\"target_kl\"] = TargetKL().opt(trial)\n",
        "        hyps[\"tensorboard_log\"] = tb_logs\n",
        "        hyps[\"policy_kwargs\"] = PolicyKWargs().opt(trial)\n",
        "        hyps[\"verbose\"] = 0\n",
        "        hyps[\"seed\"] = 1\n",
        "        hyps[\"device\"] = 'auto'\n",
        "        hyps[\"_init_setup_model\"] = True\n",
        "\n",
        "        return hyps"
      ],
      "metadata": {
        "id": "tvb-ZuNs1SyY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d1EkXTIG1ZRR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WElPkWr51b73"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *runner trainer*"
      ],
      "metadata": {
        "id": "stYYGwwu1cT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.offline as pyo\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "import json\n",
        "\n",
        "import optuna\n",
        "from sklearn import model_selection\n",
        "\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "import gym\n",
        "\n",
        "# from Trainer import *\n",
        "# from PPO_HypConfig import *\n",
        "# from Loader import *\n",
        "\n",
        "all_optimizers  =[\n",
        "    'optuna',\n",
        "    'sklearn'\n",
        "]\n",
        "\n",
        "domain = {\n",
        "    \"env_name\" : [env_name.id for env_name in gym.envs.registry.all()],\n",
        "    \"algo\" : [\"A2C\", \"DDPG\", \"DQN\", \"PPO\", \"SAC\", \"TD3\"],\n",
        "    \"optimizer\": [\"optuna\"],\n",
        "    \"sampler\": [\"BruteForceSampler\", \"CmaEsSampler\", \"GridSampler\", \"PartialFixedSampler\", \"QMCSampler\", \"RandomSampler\", \"MOTPESampler\", \"TPESampler\", \"NSGAIISampler\"],\n",
        "    \"pruner\": [\"HyperbandPruner\", \"MedianPruner\", \"NoPruner\", \"PatientPruner\", \"PercentilePruner\", \"SuccessiveHalvingPruner\", \"ThresholdPruner\"],\n",
        "}\n",
        "\n",
        "try:\n",
        "    open('previous_request.json', 'r')\n",
        "except:\n",
        "    main = {\n",
        "        \"env_name\" : \"CartPole-v1\",\n",
        "        \"algo\" : \"PPO\",\n",
        "        \"optimizer\": \"optuna\",\n",
        "        \"timesteps\": 1000,\n",
        "        \"iterations\": 10,\n",
        "        \"n_trials\": 2,\n",
        "        \"counter\" : 0,\n",
        "        \"sampler\": \"TPESampler\",\n",
        "        \"pruner\": \"MedianPruner\"\n",
        "    }\n",
        "    with open(\"previous_request.json\", \"w\") as f:\n",
        "        json.dump(main, f)\n",
        "\n",
        "\n",
        "class OptunaTuner:\n",
        "    def __init__(self, env_name, algo, directory, optimizer, ts, iterations, n_trials, counter, sampler, pruner):\n",
        "        self.env_name = env_name\n",
        "        self.algorithm = algo\n",
        "        self.directory = directory\n",
        "        self.optimizer = optimizer\n",
        "        self.timestep = ts\n",
        "        self.iterations = iterations\n",
        "        self.n_trials = n_trials\n",
        "        self.exe_number = counter\n",
        "\n",
        "        self.sampler = sampler\n",
        "        self.pruner = pruner\n",
        "\n",
        "    def create_study_db(self):\n",
        "        optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
        "        study_name = f\"study-{self.exe_number}\"  # Unique identifier of the study.\n",
        "        \n",
        "        storage_dir = f\"logs/{self.directory}\"\n",
        "        if(not os.path.exists(storage_dir)):\n",
        "            os.makedirs(storage_dir)\n",
        "        storage_name = f\"sqlite:///{storage_dir}/{study_name}.db\"\n",
        "\n",
        "        return study_name, storage_name\n",
        "    \n",
        "    def select_pruner(self):\n",
        "        if(self.pruner is None):\n",
        "            return None\n",
        "        elif(self.pruner == \"HyperbandPruner\"):\n",
        "            return optuna.pruners.HyperbandPruner(\n",
        "                    min_resource=1, max_resource=10, reduction_factor=3\n",
        "                )\n",
        "        elif(self.pruner == \"MedianPruner\"):\n",
        "            return optuna.pruners.MedianPruner(\n",
        "                    n_startup_trials=5, n_warmup_steps=30, interval_steps=10\n",
        "                )\n",
        "        elif(self.pruner == \"NoPruner\"):\n",
        "            return optuna.pruners.NopPruner()\n",
        "        elif(self.pruner == \"PatientPruner\"):\n",
        "            return optuna.pruners.PatientPruner(optuna.pruners.MedianPruner(), patience=1)\n",
        "        elif(self.pruner == \"PercentilePruner\"):\n",
        "            return optuna.pruners.PercentilePruner(\n",
        "                    25.0, n_startup_trials=5, n_warmup_steps=30, interval_steps=10\n",
        "                )\n",
        "        elif(self.pruner == \"SuccessiveHalvingPruner\"):\n",
        "            return optuna.pruners.SuccessiveHalvingPruner()\n",
        "        elif(self.pruner == \"ThresholdPruner\"):\n",
        "            return optuna.pruners.ThresholdPruner(upper=1.0, lower=0.0)\n",
        "\n",
        "    def select_sampler(self):\n",
        "        if(self.sampler is None):\n",
        "            return None\n",
        "        if(self.sampler == \"BruteForceSampler\"):\n",
        "            return optuna.samplers.BruteForceSampler()\n",
        "        elif(self.sampler == \"CmaEsSampler\"):\n",
        "            return optuna.samplers.CmaEsSampler()\n",
        "        elif(self.sampler == \"GridSampler\"):\n",
        "            return optuna.samplers.GridSampler()\n",
        "        elif(self.sampler == \"PartialFixedSampler\"):\n",
        "            return optuna.samplers.PartialFixedSampler()\n",
        "        elif(self.sampler == \"QMCSampler\"):\n",
        "            return optuna.samplers.QMCSampler()\n",
        "        elif(self.sampler == \"RandomSampler\"):\n",
        "            return optuna.samplers.RandomSampler()\n",
        "        elif(self.sampler == \"MOTPESampler\"):\n",
        "            return optuna.samplers.MOTPESampler()\n",
        "        elif(self.sampler == \"TPESampler\"):\n",
        "            return optuna.samplers.TPESampler()\n",
        "        elif(self.sampler == \"NSGAIISampler\"):\n",
        "            return optuna.samplers.NSGAIISampler()\n",
        "        \n",
        "    \n",
        "    def study_loader(self):\n",
        "        study_name = f\"study-{self.exe_number}\"  # Unique identifier of the study.\n",
        "        storage_dir = f\"logs/{self.directory}\"\n",
        "        storage_name = f\"sqlite:///{storage_dir}/{study_name}.db\"\n",
        "\n",
        "        study = optuna.load_study(\n",
        "            study_name=study_name,\n",
        "            storage=storage_name\n",
        "        )\n",
        "\n",
        "        trials_df = study.trials_dataframe()\n",
        "        print(trials_df.head())\n",
        "\n",
        "        return study\n",
        "    \n",
        "    def study_summaries(self):\n",
        "        study_name = f\"study-{self.exe_number}\"  # Unique identifier of the study.\n",
        "        storage_dir = f\"logs/{self.directory}\"\n",
        "        storage_name = f\"sqlite:///{storage_dir}/{study_name}.db\"\n",
        "\n",
        "        summary = optuna.get_all_study_summaries(\n",
        "            storage_name, include_best_trial=True\n",
        "        )\n",
        "        print(summary)\n",
        "        for i,j in enumerate(summary):\n",
        "            print(f\"-----------STUDY-{i}-----------\")\n",
        "            for attribute in [attr for attr in dir(j) if not attr.startswith('__')]:\n",
        "                if(attribute==\"system_attrs\" or attribute==\"user_attrs\"):\n",
        "                    continue\n",
        "                print(attribute, ':', getattr(j, attribute))\n",
        "            print()\n",
        "        pass\n",
        "    \n",
        "    def return_study(self, bool_store_study=True, bool_select_pruner=True, bool_select_sampler=True):\n",
        "        study_name, storage_name, selected_pruner, selected_sampler = [None] * 4\n",
        "        if(bool_store_study):\n",
        "            study_name, storage_name = self.create_study_db()\n",
        "\n",
        "        if(bool_select_pruner):\n",
        "            selected_pruner = self.select_pruner()\n",
        "        \n",
        "        if(bool_select_sampler):\n",
        "            selected_sampler = self.select_sampler()\n",
        "        \n",
        "        lie = storage_name is not None\n",
        "\n",
        "        study = optuna.create_study(\n",
        "                storage=storage_name,\n",
        "                sampler=selected_sampler,\n",
        "                pruner=selected_pruner,\n",
        "                study_name=study_name,\n",
        "                direction='maximize',\n",
        "                load_if_exists=lie,\n",
        "                directions=None\n",
        "            )\n",
        "        return study\n",
        "\n",
        "    def call_optuna(self, store_study=True):\n",
        "        study = self.return_study()\n",
        "        study.optimize(self.objective, n_trials=self.n_trials)\n",
        "        \n",
        "    def objective(self, trial):\n",
        "        m = MegaTrainer(self.env_name, self.algorithm)\n",
        "        counter = m.learn(\n",
        "            self.timestep,\n",
        "            self.iterations,\n",
        "            MegaHandler().request_next_HypConfig(trial, m.env, f\"logs/{self.directory}\"),\n",
        "            self.directory,\n",
        "            trial=trial\n",
        "        )\n",
        "\n",
        "        m_load  = MegaLoader(self.env_name, self.algorithm, self.directory)\n",
        "        return m_load.load(counter, self.n_trials)[\"total_return\"]\n",
        "\n",
        "    def visul(self):\n",
        "        study = self.study_loader()\n",
        "        if(not \"_optimization_history.py\"):\n",
        "            pyo.plot(optuna.visualization.plot_optimization_history(study))\n",
        "        \n",
        "        if(not \"_timeline.py\"):\n",
        "            pyo.plot(optuna.visualization.plot_timeline(study))\n",
        "        \n",
        "        if(not \"_slice.py\"):\n",
        "            pyo.plot(optuna.visualization.plot_slice(study))\n",
        "        \n",
        "        if(not \"_pareto_front.py\"): ## this is used for multi-objective study\n",
        "            pyo.plot(optuna.visualization.plot_pareto_front(study))\n",
        "        \n",
        "        if(\"_param_importances.py\"):\n",
        "            pyo.plot(optuna.visualization.plot_param_importances(study))\n",
        "        \n",
        "        if(not \"_parallel_coordinate.py\"):\n",
        "            pyo.plot(optuna.visualization.plot_parallel_coordinate(study))\n",
        "            \n",
        "        if(not \"_intermediate_values.py\"):\n",
        "            pyo.plot(optuna.visualization.plot_intermediate_values(study))\n",
        "        \n",
        "        if(not \"_edf.py\"):\n",
        "            pyo.plot(optuna.visualization.plot_edf(study))\n",
        "            \n",
        "        if(not \"_contour.py\"):\n",
        "            pyo.plot(optuna.visualization.plot_contour(study))\n",
        "\n",
        "class HyperPilotRL:\n",
        "    def __init__(self, env_name, algo, optimizer, timesteps, iterations, n_trials, counter, sampler=None, pruner=None):\n",
        "        self.env_name = env_name\n",
        "        self.algorithm = algo\n",
        "        if(sampler is None):\n",
        "            sampler = \"TPESampler\"\n",
        "        if(pruner is None):\n",
        "            pruner = \"MedianPruner\"\n",
        "        self.directory = f\"{optimizer}/{counter}/{env_name}/{algo}/{sampler}/{pruner}\"\n",
        "        self.optimizer = optimizer\n",
        "        self.timestep = timesteps\n",
        "        self.iterations = iterations\n",
        "        self.n_trials = n_trials\n",
        "        self.execution_number = counter\n",
        "        self.sampler = sampler\n",
        "        self.pruner = pruner\n",
        "\n",
        "        if(self.optimizer == \"optuna\"):\n",
        "            self.o = OptunaTuner(env_name, algo, self.directory, optimizer, timesteps, iterations, n_trials, counter, sampler, pruner)\n",
        "            # o.call_optuna()\n",
        "            # # o.study_summaries()\n",
        "            # # o.visul()\n",
        "        elif(self.optimizer == \"default\"):\n",
        "            pass\n",
        "        else:\n",
        "            raise NameError(\"Enter optimizer is not supported.\\nAvailable optimizers are\", all_optimizers)\n",
        "\n",
        "        ## optimizer = ['optuna']\n",
        "\n",
        "    def learning_curve():\n",
        "        pass\n",
        "\n",
        "    def cutoff_learn_according_to_max_time_allocation():\n",
        "        pass\n",
        "\n",
        "    def hyp_search(self):   # for all implemented optimizers go through this to do \"hyperparameter search part\"\n",
        "        if(self.optimizer == \"optuna\"):\n",
        "            self.o.call_optuna()\n",
        "    \n",
        "    def summary(self):\n",
        "        if(self.optimizer == \"optuna\"):\n",
        "            self.o.study_summaries()\n",
        "    \n",
        "    def visualization(self):\n",
        "        if(self.optimizer == \"optuna\"):\n",
        "            self.o.visul()\n",
        "\n",
        "    def get_study_data(self):\n",
        "        if(self.optimizer == \"optuna\"):\n",
        "            return self.o.return_study()\n",
        "\n",
        "if(__name__==\"__main__\"):\n",
        "    data = {}\n",
        "    with open(\"previous_request.json\", \"r\") as f:\n",
        "        data = json.loads(f.read())\n",
        "        \n",
        "        for i, j in data.items():\n",
        "            if(i!=\"env_name\" and i in domain):\n",
        "                print(f\"{i}\\n    {domain[i]}\")\n",
        "            while(True):\n",
        "                inp = input(f\"Current value of {i} is {j}. Change/Pass? \")\n",
        "                if(inp==\"\"):\n",
        "                    break\n",
        "\n",
        "                if(i in domain):\n",
        "                    if(inp!=\"\" and inp in domain[i]):\n",
        "                        data[i] = inp\n",
        "                        break\n",
        "                else:\n",
        "                    data[i] = int(inp)\n",
        "                    break\n",
        "    \n",
        "        data[\"counter\"] += 1\n",
        "\n",
        "        print(\"\\n\\n\\n\\n\")\n",
        "        print(\"----------------------------------------------------------------\")\n",
        "        print(data)\n",
        "        print(\"----------------------------------------------------------------\")\n",
        "\n",
        "        with open(\"previous_request.json\", \"w\") as f:\n",
        "            json.dump(data, f)\n",
        "\n",
        "    # h = HyperPilotRL(\n",
        "    #     env_name=\"CartPole-v1\",\n",
        "    #     algo=\"PPO\",\n",
        "    #     optimizer=\"optuna\",\n",
        "    #     timesteps=1000,\n",
        "    #     iterations=10,\n",
        "    #     n_trials=2,\n",
        "    #     counter=1030,\n",
        "    #     sampler=\"RandomSampler\",\n",
        "    #     pruner=\"HyperbandPruner\"\n",
        "    # )\n",
        "\n",
        "    h = HyperPilotRL(\n",
        "        **data\n",
        "    )\n",
        "    h.hyp_search()\n",
        "    # h.summary()\n",
        "    # h.visualization()\n",
        "    # h.get_study_data()\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT9vOut61kNh",
        "outputId": "b3e6a98c-e4b9-48fd-ca5e-cfb96809ca8c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:421: UserWarning:\n",
            "\n",
            "\u001b[33mWARN: The `registry.all` method is deprecated. Please use `registry.values` instead.\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current value of env_name is CartPole-v1. Change/Pass? \n",
            "algo\n",
            "    ['A2C', 'DDPG', 'DQN', 'PPO', 'SAC', 'TD3']\n",
            "Current value of algo is PPO. Change/Pass? \n",
            "optimizer\n",
            "    ['optuna']\n",
            "Current value of optimizer is optuna. Change/Pass? \n",
            "Current value of timesteps is 1000. Change/Pass? \n",
            "Current value of iterations is 1. Change/Pass? \n",
            "Current value of n_trials is 2. Change/Pass? \n",
            "Current value of counter is 7. Change/Pass? \n",
            "sampler\n",
            "    ['BruteForceSampler', 'CmaEsSampler', 'GridSampler', 'PartialFixedSampler', 'QMCSampler', 'RandomSampler', 'MOTPESampler', 'TPESampler', 'NSGAIISampler']\n",
            "Current value of sampler is TPESampler. Change/Pass? \n",
            "pruner\n",
            "    ['HyperbandPruner', 'MedianPruner', 'NoPruner', 'PatientPruner', 'PercentilePruner', 'SuccessiveHalvingPruner', 'ThresholdPruner']\n",
            "Current value of pruner is MedianPruner. Change/Pass? \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "{'env_name': 'CartPole-v1', 'algo': 'PPO', 'optimizer': 'optuna', 'timesteps': 1000, 'iterations': 1, 'n_trials': 2, 'counter': 8, 'sampler': 'TPESampler', 'pruner': 'MedianPruner'}\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-04-06 09:17:07,325]\u001b[0m A new study created in RDB with name: study-8\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A new study created in RDB with name: study-8\n",
            "A new study created in RDB with name: study-8\n",
            "A new study created in RDB with name: study-8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning:\n",
            "\n",
            "\u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning:\n",
            "\n",
            "\u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:256: DeprecationWarning:\n",
            "\n",
            "\u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "\n",
            "\u001b[32m[I 2023-04-06 09:17:11,555]\u001b[0m Trial 0 finished with value: 69.0 and parameters: {'learning_rate': 8.462783026457984e-05, 'n_steps': 12, 'batch_size': 7, 'n_epochs': 10, 'gamma': 0.979343139154399, 'bvtradeoff': 0.9886222900413673, 'clip_range': 0.20576760213765577, 'clip_range_vf': 0.1894580339832997, 'ent_coef': 0.004002772606397064, 'vf_coef': 0.5593721031506733, 'max_grad_norm': 0.8272176298857028, 'target_kl': 0.045838434413046164}. Best is trial 0 with value: 69.0.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 0 finished with value: 69.0 and parameters: {'learning_rate': 8.462783026457984e-05, 'n_steps': 12, 'batch_size': 7, 'n_epochs': 10, 'gamma': 0.979343139154399, 'bvtradeoff': 0.9886222900413673, 'clip_range': 0.20576760213765577, 'clip_range_vf': 0.1894580339832997, 'ent_coef': 0.004002772606397064, 'vf_coef': 0.5593721031506733, 'max_grad_norm': 0.8272176298857028, 'target_kl': 0.045838434413046164}. Best is trial 0 with value: 69.0.\n",
            "Trial 0 finished with value: 69.0 and parameters: {'learning_rate': 8.462783026457984e-05, 'n_steps': 12, 'batch_size': 7, 'n_epochs': 10, 'gamma': 0.979343139154399, 'bvtradeoff': 0.9886222900413673, 'clip_range': 0.20576760213765577, 'clip_range_vf': 0.1894580339832997, 'ent_coef': 0.004002772606397064, 'vf_coef': 0.5593721031506733, 'max_grad_norm': 0.8272176298857028, 'target_kl': 0.045838434413046164}. Best is trial 0 with value: 69.0.\n",
            "Trial 0 finished with value: 69.0 and parameters: {'learning_rate': 8.462783026457984e-05, 'n_steps': 12, 'batch_size': 7, 'n_epochs': 10, 'gamma': 0.979343139154399, 'bvtradeoff': 0.9886222900413673, 'clip_range': 0.20576760213765577, 'clip_range_vf': 0.1894580339832997, 'ent_coef': 0.004002772606397064, 'vf_coef': 0.5593721031506733, 'max_grad_norm': 0.8272176298857028, 'target_kl': 0.045838434413046164}. Best is trial 0 with value: 69.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-04-06 09:17:14,105]\u001b[0m Trial 1 finished with value: 74.0 and parameters: {'learning_rate': 0.004125758760179427, 'n_steps': 11, 'batch_size': 7, 'n_epochs': 9, 'gamma': 0.911605188896017, 'bvtradeoff': 0.9203354061073798, 'clip_range': 0.1448614265775217, 'clip_range_vf': 0.24802424583468188, 'ent_coef': 0.007762473532262381, 'vf_coef': 0.5315651676422082, 'max_grad_norm': 0.6673555767866619, 'target_kl': 0.04305242419817512}. Best is trial 1 with value: 74.0.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1 finished with value: 74.0 and parameters: {'learning_rate': 0.004125758760179427, 'n_steps': 11, 'batch_size': 7, 'n_epochs': 9, 'gamma': 0.911605188896017, 'bvtradeoff': 0.9203354061073798, 'clip_range': 0.1448614265775217, 'clip_range_vf': 0.24802424583468188, 'ent_coef': 0.007762473532262381, 'vf_coef': 0.5315651676422082, 'max_grad_norm': 0.6673555767866619, 'target_kl': 0.04305242419817512}. Best is trial 1 with value: 74.0.\n",
            "Trial 1 finished with value: 74.0 and parameters: {'learning_rate': 0.004125758760179427, 'n_steps': 11, 'batch_size': 7, 'n_epochs': 9, 'gamma': 0.911605188896017, 'bvtradeoff': 0.9203354061073798, 'clip_range': 0.1448614265775217, 'clip_range_vf': 0.24802424583468188, 'ent_coef': 0.007762473532262381, 'vf_coef': 0.5315651676422082, 'max_grad_norm': 0.6673555767866619, 'target_kl': 0.04305242419817512}. Best is trial 1 with value: 74.0.\n",
            "Trial 1 finished with value: 74.0 and parameters: {'learning_rate': 0.004125758760179427, 'n_steps': 11, 'batch_size': 7, 'n_epochs': 9, 'gamma': 0.911605188896017, 'bvtradeoff': 0.9203354061073798, 'clip_range': 0.1448614265775217, 'clip_range_vf': 0.24802424583468188, 'ent_coef': 0.007762473532262381, 'vf_coef': 0.5315651676422082, 'max_grad_norm': 0.6673555767866619, 'target_kl': 0.04305242419817512}. Best is trial 1 with value: 74.0.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KludHA8C1rKN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}